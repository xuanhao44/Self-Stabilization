# 6.1 Digital Clock Synchronization

The digital clock-synchronization problem is defined for a system of $n$ identical processors connected to a global common clock pulse. Each processor maintains a digital clock value. In every pulse each processor executes a step in which it reads the value of its neighbors’ clocks and uses these values to calculate its new clock value. In other words, a pulse triggers read operations by all the processors, and once the read operations by all the processors are finished, the processors choose new clock values and change state accordingly. The term *lock-step* is sometimes used for such synchronous processor activation.

Starting in a configuration in which every processor has an arbitrary clock value, the processors should reach a safe configuration $c$ in which: (1) all the clock values are identical, and (2) in every execution that starts in $c$, the clocks are incremented by one in every pulse. Note that the fact that the processors are identical does not contradict the existence of a deterministic algorithm for the task, since we are interested in reaching a symmetric configuration in which all the clock values are identical (unlike, e.g., the leader-election task).

![figure_6.1](images/figure_6.1.png)

The first self-stabilizing algorithm for synchronizing digital clocks that we describe uses an unbounded number of clock values. The code for the algorithm appears in figure 6.1. Let $max$ be the maximal clock value that a processor $P_i$ reads from a neighbor during a certain pulse. $P_i$ assigns the value $max+1$ to its own clock. This simple algorithm is self-stabilizing, since from any initial configuration that follows at least $d$ pulses (where $d$ is the diameter of the system), the clocks are synchronized and incremented by one in any later pulse.

Let $P_m$ be a processor with the maximal clock value in the first configuration. The correctness of the algorithm can be easily proven by an induction on the distance from $P_m$; we assume that, following $i$ pulses, every processor of distance $i$ from $P_m$ holds the (current) maximal clock value. The algorithm uses unbounded clocks, which is a serious drawback in self-stabilizing systems. Every implementation of the algorithm must use bounded memory for the clock (e.g., a sixty-four-bit register). As already remarked, the assumption that a sixty-four-bit digital clock is "unbounded" for every implementation because it will take $2^{64}$ time units to reach the upper bound (which is large enough for every possible application) does not hold in the design of self-stabilizing systems, where a single transient fault may cause the clock immediately to reach the maximal clock value.

We discuss two self-stabilizing algorithms that use bounded clock values. The first bounded algorithm is almost identical to the unbounded algorithm, the only difference being that the clocks are incremented modulo $M$ where $M > (n + 1)d$ (note that for ease of description the values we choose for $M$ are not the minimal possible value). The code of the algorithm appears in figure 6.2.

![figure_6.2](images/figure_6.2.png)

![figure_6.3](images/figure_6.3.png)

Note that, if the system is initialized in a configuration in which the values of the clocks are less than $M − d$, then the clocks are synchronized before the modulo operation is applied. Once the clocks are synchronized, the value zero is assigned to the clocks simultaneously in the pulse immediately after a configuration in which the clock values are all $M − 1$. The correctness proof for the first algorithm uses the pigeonhole principle, showing that, in any configuration, there must be two clock values $x$ and $y$ such that y$ − x ≥ d + 1$ and there is no other clock value between. Furthermore, since each processor chooses an existing clock value and increments it by one, it holds at the first pulse of the execution that no clock can be assigned a value greater than the value of $x +1$ and less than the value of $y +1$. Similarly, until $y$ is incremented to $M$, it holds that, following the $i$ th pulse, no clock value is greater than $x + i$ and smaller than $y + i$. Thus, after $M − y + 1$ pulses, the system reaches a configuration in which there is no clock value that is greater than $M −d$. In the next d rounds, the maximal clock value propagates, and the system reaches a configuration in which all the clocks are synchronized.

The other bounded algorithm, in figure 6.3, uses the minimal, instead of the maximal, clock value read by a processor. For this version, it is sufficient to use $M > 2d$. As in previous algorithms, the number of distinct clock values can only be reduced during the execution. Actually, it is shown that the number of clock values is reduced to a single clock value. Two cases are considered. In the first, no processor assigns zero to its clock during the first $d$ pulses. A simple induction on the distance from a processor that holds the minimal clock value in the first configuration proves that, following $i ≤ d$ pulses, all neighbors at distance $i$ from this processor have the minimal clock value in the system. In the other case, a processor assigns zero to its clock during the first d pulses. Here it is easy to see that, in $d$ pulses after this assignment, a configuration $c$ is reached such that there is no clock value in $c$, that is greater than $d$. In the first $d$ pulses that follow $c$, no processor assigns zero to its clock. Thus, the arguments for the first case hold.

## Digital Clocks with a Constant Number of States

One may wonder whether the number of clock values be further reduced. Can this number be a fixed constant that is related to neither the diameter of the system nor the number of processors in the system? The following elegant lower bound on the number of states per processor proves that there is no uniform digital clock-synchronization algorithm that uses only a constant number of states per processor. To prove the lower bound, we restrict our attention to systems with ring communication graphs. This is a special case of general communication graph systems; therefore, a lower bound for this special case implies a lower bound for the general communication graph case.

To present the main ideas of the lower bound, we start in a restricted case in which a processor can read only the clock of a subset of its neighbors, and we prove a lower bound for a unidirected ring. In a unidirected ring every processor has a left and a right neighbor. The left and right neighbor relation is global in the following sense: if $P_i$ is the left neighbor of $P_j$ then $P_j$ is the right neighbor of $P_i$. In a unidirected ring a processor can read the state of its left neighbor.

Given any self-stabilizing digital clock-synchronization algorithm for a unidirected ring, denote the transition function of every processor by $f$. Each transition is denoted by $s^{t+1}_i = f(s^t_{i−1},s^t_i)$, where $s^t_i$ and $s^t_{i−1}$ are the states of $P_i$ and its left neighbor, respectively, at time $t$, and $s^{t+1}_i$ is the state of $P_i$ at time $t + 1$. Assume that the number of states of every processor is a constant; in other words, the number of possible states is not a function of the number of the processors in the ring. Let $|S|$ be the constant number of states of a processor. The idea is to choose a sufficiently large ring for which the given algorithm will never stabilize. The proof shows that a configuration exists for a sufficiently large ring such that the states of the processors rotate: in every step, the state of every processor is changed to the state of its right processor.

Figure 6.4 depicts our next argument. Let $s_1$ and $s_2$ be two states in $S$; e.g., the first two states according to some arbitrary ordering of the states. Use $s_1$ and $s_2$ to construct an infinite sequence of states such that $s_{l+2} = f(s_l,s_{l+1})$ (the upper two lines of figure 6.4 are such a sequence). There must be a sequence of states $s_j$, $s_{j+1}$, ···, $s_{k−1}$, $s_k$ — that is, a subset of the above infinite sequence — such that $f(s_{k−1}, s_k) = s_j$ and $f(s_k , s_j) = s_{j+1}$ and $k \geq j+3$; or, equivalently, $s_{k+1} = s_j$ and $s_{k+2} = s_{j+1}$. Consider the sequence of pairs $(s_1, s_2)$, $(s_5,s_6)$, $(s_9, s_{10})$, ···, $(s_{4(i−1)+1}, s_{4(i−1)+2})$ ···. Any sequence of $|S|^2 + 1$ such pairs has at least one pair $(s_j , s_{j+1})$ that appears more than once. Thus, any segment of $2(|S|^2 + 1)$ states in the infinite sequence can be used in our proof.

![figure_6.4](images/figure_6.4.png)

Now, we are convinced that there is a sequence $s_j, s_{j+1}, ··· , s_{k−1}, s_k$ in which the combination $s_{k+1}, s_{k+2}$ and the combination $s_j, s_{j+1}$ are identical. Therefore, it holds that $f(s_{k−1}, s_k) = s_j$ and $f(s_k, s_j) = s_{j+1}$. Now construct a unidirected ring of processors using the sequence $s_j, s_{j+1}, ···, s_{k−1}, s_k$, where the processor in state $s_k$ is the left neighbor of the processor in state $s_j$ (see the lower portion of figure 6.4). Each processor uses its own state and the state of its left neighbor to compute the next state; in accordance with our construction, the state $s_{j+i}$ is changed to $s_{j+i+1}$ for every $0 \leq i < k − j$, and the state $s_k$ is changed to $s_j$. We conclude that, in each pulse, the states are rotated one place to the left. Note that the above is true in an infinite execution starting in the configuration defined above.

Is it possible that such an infinite execution will stabilize? Since the states of the processors encodes the clock value and the set of states is not changed during an infinite execution (it just rotates around the ring), we must assume that all the states encode the same clock value. On the other hand, the clock value must be incremented in every pulse. This is impossible, since the set of states is not changed during the infinite execution.

In the more complicated case of bidirectional rings, the lower-bound proof uses a similar approach. We denote a transition by $s^{t+1}_i = f(s^t_{i−1}, s^t_i, s^t_{i+1})$. In this case, there must exist a sequence of states $s_1, s_2, ···, s_k$ of length $O(|S|^3)$ such that, for every $i > 2$, $f(s_{i−1}, s_i, s_{i+1}) = s_{i+2}$ and $f(s_{k−2}, s_{k−1}, s_k) = s_1$, $f(s_{k−1}, s_k, s_1) = s_2$, $f(s_k, s_1, s_2) = s_3$, where $k \geq 3$. The reason is that the number of state combinations, $s_{i−1}, s_i, s_{i+1}$, is bounded by $|S|^3$. The proof is completed by the rotation argument, where every activation results in a double rotation.

Note that a randomized self-stabilizing algorithm that uses a constant number of clocks does exist. At each pulse, a processor $P_i$ randomly chooses a clock value $clock$ that is either its own clock value or the clock value of a neighbor. $P_i$ assigns $(clock + 1)$ mod $M$ to ${clock}_i$. Again, this choice ensures that the number of distinct clock values can only be reduced during the execution. A simple *sl-game* winning strategy can be used. Choose a particular node $P_l$; if each processor repeatedly chooses a clock value of a neighbor in a shortest path to $P_l$, then the system reaches a safe configuration within $d$ pulses. Thus, in every *infinite* execution, the algorithm stabilizes with probability 1.

Until this point, we have studied self-stabilizing algorithms for digital clock synchronization. In the following sections, we present algorithms that cope with other types of faults in addition to transient faults.
