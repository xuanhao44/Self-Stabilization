# 4.1 Token-Passing: Converting a Central Daemon to read/write

The literature in self-stabilization is rich in algorithms that assume the existence of powerful schedulers, the *distributed daemon* or the *central daemon*. The distributed daemon activates the processors by repeatedly selecting a set of processors and activating them simultaneously to execute a computation step. Each processor executes the next computation step as defined by its state just prior to this activation. We use read and write operations to describe precisely the meaning of the simultaneous execution of the computation steps. Simultaneous execution starts with all the activated processors reading the states of their neighbors. Once every processor in the set has finished reading, all the processors write a new state (change state). Only then does the scheduler choose a new set of processors to be activated. Note that no non-activated processor changes its state. The central daemon is a special case of the distributed daemon in which the set of activated processors consists of exactly one processor. Note that a synchronous system in which all the processors are activated in every step is also a special case of a system with a distributed daemon.

What are the reasons for using the above special settings in the literature on self-stabilization? One reason is the choices Dijkstra made in the first work in the field. Another is the inherent subtlety of designing self-stabilizing algorithms, which can be started in any possible configuration. The assumption of the existence of a daemon enables the designer to consider only a subset of the possible execution set. In particular, there is no need to use internal variables to store the values read from the communication registers (or to consider the values of these internal variables as subject to corruption by transient faults as well).

The great importance of relaxing the assumption concerning the schedule pattern is obvious. An algorithm designed to work in read/write atomicity can be used in any system in which there exists a central or a distributed daemon, while an algorithm designed for a system with a central or a distributed daemon cannot be used in a system that supports only read/write atomicity. The above facts are our motivation for designing a compiler that receives as an input a self-stabilizing algorithm $\mathcal{AL}$ for some task $\mathcal{T}$ that is designed for systems with the central or distributed daemon, and outputs a version of $\mathcal{AL}$ for task $\mathcal{T}$ for systems that support only read/write atomicity. An algorithm designed to stabilize in the presence of a distributed daemon must stabilize in a system with a central daemon — one possible policy of the distributed daemon is to activate one processor at a time, and an algorithm designed to stabilize under any scheduling policy of the distributed daemon must stabilize when one processor is activated at a time. In this section, we suggest distributively implementing a central daemon by a self-stabilizing mutual exclusion or token-passing algorithm. When a processor enters the critical section in the mutual exclusion algorithm, or when a processor holds the token in the token-passing algorithm, it reads the states of its neighbors and changes state accordingly. Once the processor changes its state, it exits the critical section or passes the token.

The algorithm is designed for a system with an arbitrary communication graph and a single special processor. In what follows, we show how the assumption on the existence of a special processor can be achieved by using a self-stabilizing leader-election algorithm.

We use the algorithm presented in section 2.7, which uses the fair-composition technique to compose the spanning-tree-construction algorithm of section 2.5 with the read/write atomicity version of Dijkstra’s mutual exclusion algorithm in section 2.6. Once the spanning-tree construction algorithm stabilizes, a virtual ring forming the Euler tour on the spanning tree is distributively defined. Moreover, the root of the tree acts as the special processor in the ring. Thus, when the mutual exclusion algorithm stabilizes, it is the case that exactly one processor may enter the critical section in every configuration. A processor that enters the critical section of the mutual exclusion algorithm reads the state of its neighbors (or the portion of the state that is communicated to it through the communication registers), changes state and new state to its communication registers, and then exits the critical section. Note that every time a processor enters the critical section it executes a constant number of steps and then exits. Therefore, the convergence of the mutual exclusion algorithm to a safe configuration is still guaranteed.

Our compiler receives as input an algorithm $\mathcal{AL}$ that stabilizes in the presence of a central daemon and composes it with a self-stabilizing mutual exclusion algorithm that schedules the operations of the processors. In an arbitrary initial configuration of the composed algorithm, it is possible that more than a single processor is scheduled to execute a step in $\mathcal{AL}$. Therefore, while the mutual exclusion algorithm stabilizes, it is possible that two neighboring processors will change state simultaneously. Therefore, $\mathcal{AL}$ may not converge to a safe configuration until the mutual exclusion algorithm stabilizes. In fact, however, we are not interested in its exact behavior during this period, since it does not interfere with the activity of the mutual exclusion algorithm. When the mutual exclusion algorithm reaches a safe configuration, the state portion of the processors that is related to $\mathcal{AL}$ is arbitrary. Fortunately, $\mathcal{AL}$ converges to a safe configuration from any arbitrary configuration when it is activated by the central daemon.
