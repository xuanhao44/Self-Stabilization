# 6.2 Stabilization in Spite of Napping

We are interested in clock synchronization algorithms that can tolerate hybrid faults: these should work starting from an arbitrary initial configuration and should tolerate processors that exhibit faults during and after convergence.

Several self-stabilizing algorithms that synchronize digital clocks in the presence of (permanent) faults are known. An algorithm that tolerates both transient and permanent faults is, of course, more resilient to faults than an algorithm that tolerates only one of these types of faults.

The first set of results concerns a complete communication graph in which each processor can communicate with all other processors. A self-stabilizing clock synchronization algorithm copes with permanent *napping* faults if processors can repeatedly stop and (possibly) resume operation during and after the convergence of the algorithm to a safe configuration. Note that napping faults are more severe than crash failures, since a crashed processor can be seen as a napping processor that does not resume operation. A clock synchronization algorithm that copes with transient and napping faults is called a *wait-free self-stabilizing* clock-synchronization algorithm. A non-faulty processor $P_i$ synchronizes its clock even when all $n−1$ other processors are faulty; in other words, $P_i$ does not wait for other processors to participate in the algorithm. What is the meaning of synchronization when $n−1$ processors are faulty? How can the single operational processor be synchronized with itself? The answer to these questions is simple: the synchronization algorithm should ensure that each non-faulty operating processor ignores the faulty processors and increments its clock value by one in every pulse.

A processor $P_i$ adjusts its clock in a certain step if the value of its clock is not incremented by one in this step. The requirements of an algorithm that is both self-stabilizing and copes with napping faults are called $adjustment$ and $agreement$. We require that a fixed integer $k$ exists such that, starting in an arbitrary configuration, once a processor $P_i$ has been working correctly for at least $k$ time units and as long as $P_i$ continues to work, the following $adjustment$ and $agreement$ properties hold:

- $adjustment$: $P_i$ does not adjust its clock, and
- $agreement$: $P_i$'s clock agrees with the clock of every other processor that has also been working correctly for at least $k$ time units

Because a working processor must synchronize its clock in a fixed amount of time regardless of the actions of the other processors and because the system can be started in an arbitrary configuration, such algorithms are called *wait-free self-stabilizing* algorithms.

Note that trivial algorithms that fulfill only one of the above requirements exist. An algorithm in which a processor increments its clock by one in every pulse without communicating with its neighbors fulfills the adjustment requirement, and an algorithm in which the clock value of every processor is always 0 fulfills the agreement requirement.

Does an algorithm that fulfills both requirements exist? We first present a simple *wait-free self-stabilizing* algorithm with $k=1$ that uses unbounded clocks: a (non-crashed) processor reads the clocks of all the other processors in every step, chooses the maximal clock value in the system, denoted by $x$, and assigns $x+1$ to its clock.

Starting in an arbitrary configuration, immediately after an execution of a step by a processor $P_i$, the clock of $P_i$ holds the maximal clock value. Hence as long as $P_i$ does not crash, it does not adjust its clock; in other words, $P_i$ increments its clock by one in every pulse. Moreover, the clock of $P_i$ and every other processor that is not crashed hold the maximal clock value in the system. Thus, the clocks of all non-crashed processors that executed $k=1$ steps agree. Does the algorithm tolerate transient faults as well as napping faults? Transient faults may cause the system to reach an arbitrary configuration $c$ with arbitrary clock values. Still, after this arbitrary configuration $c$, the clock of every processor $P_i$ holds the maximal clock value immediately after each of $P_i$'s steps.

At this point, we already know that self-stabilizing algorithms that use bounded memory are more attractive than self-stabilizing algorithms that use unbounded memory. One might suggest a similar technique for bounded clocks. Instead of incrementing the clock values by one, an identical algorithm that increments the clock values by one modulo some integer $M$ might be used. Unfortunately, this suggestion will not work, since the clock of a napping processor that holds $M−1$ may cause all the active processors repeatedly to assign zero to their clocks and to violate the adjustment requirement for every possible integer $k$.

We now present a technique for a wait-free self-stabilizing clock synchronization algorithm that uses a bounded clock value $M$ and a bounded number of states for a processor. The idea is to use a mechanism for identifying crashed processors and ignoring their clock value. Every two processors $P_i$ and $P_j$ have an "arrow" indicating which of them was recently active. In every step it executes, $P_i$ makes sure that it is not *behind* $P_j$. Using the analogy of the arrow, $P_i$ makes sure that it is not the tail of the arrow by trying to direct the arrow toward itself, while $P_j$ does the same from the other side. Immediately after both processors execute a step, the arrow has no direction; i.e., $P_i$ is not behind $P_j$ and $P_j$ is not behind $P_i$.

Interestingly, there is a simple self-stabilizing implementation of this arrow. The implementation is based on the concept of the famous *scissors*, *paper*, *stone* children’s game in which two children randomly choose an element in the set scissors, paper and stone. Every time they chose different elements a winner is declared according to the following rules: it is possible to wrap a stone with paper, to break scissors with a stone, and to cut paper with scissors.

Each processor $P_i$ has an ${order}_{ij}$ variable for each neighbor $P_j$ that holds a value from {0, 1, 2}. $P_i$ writes in ${order}_{ij}$ and every processor can read ${order}_{ij}$. The arrow has no direction when the value of ${order}_{ij}$ is equal to the value of ${order}_{ji}$. When ${order}_{ij}$ is not equal to ${order}_{ji}$, the arrow is directed. In this case, we define the behind relation by the missing value $x$, which is the value from {0, 1, 2} such that $x = {order}_{ij}$ and $x = {order}_{ji}$. The convention we use is that the value that follows $x$ in the clockwise direction is behind the other value. In other words, $P_i$ is behind $P_j$ if and only if (${order}_{ij} + 1) \mod 3 = {order}_{ji}$.

$P_i$ identifies all napping processors $P_j$ by incrementing (${order}_{ij} + 1$) mod 3 as long as $P_j$ is not behind $P_i$. To get a clearer understanding of this mechanism, we observe the steps of two processors $P_i$ and $P_j$ in a specific execution. This execution starts in a configuration in which the arrow has no direction and continues with a step of $P_i$ and $P_j$. In each step, both processors try to signal each other that they are executing a step by directing the arrow toward themselves. Thus, $P_i$ increments ${order}_{ij}$ by 1 modulo 3 and $P_j$ does the same. This results in a configuration in which the arrow is not directed. The same holds as long as both processors continue to execute steps. Immediately after the first time in which only one of the two processors, say $P_i$, executes a step, $P_j$ is behind $P_i$. When $P_j$ resumes operation, it finds that it is behind $P_i$ and increments ${order}_{ji}$ by 1 modulo 3. Note that, if $P_i$ executes a step, too, $P_i$ does not increment ${order}_{ij}$ since $P_j$ is behind $P_i$.

This order mechanism is used for selecting (bounded) clock values. The clock values of the napping processors are ignored. At each pulse, $P_i$ reads the order variables between every two processors $P_j$ and $P_l$, $1 \leq j$, $l \leq n$ in the system. Let $\mathcal{NB}$ be the set of processors that are not behind any other processor according to the order variables $P_i$ read. If $\mathcal{NB}$ is not empty, then $P_i$ chooses the maximal clock value $x$ among the clock values of the processors in $\mathcal{NB}$ and assigns $(x + 1) \mod M$ to its clock. Otherwise, $P_i$ does not change its clock value.

---

> THEOREM 6.1: The above algorithm is a wait-free self-stabilizing clock-synchronization algorithm with $k=2$.

*Proof:*

The proof shows that, starting with any values in the $order$ variables and the $clock$ variables, the algorithm meets the adjustment and agreement requirements.

First note that all processors that take a step at the same pulse, see the same view, and compute the same $\mathcal{NB}$. We must show that, if any processor $P_i$ executes more than $k=2$ successive steps, then the agreement and adjustment requirements hold following its second step.

Assume $P_i$ executes more than $k$ successive steps. Our first observation is that $\mathcal{NB}$ is not empty following the first step of $P_i$. Moreover, while $P_i$ continues to execute steps without stopping, it remains in $\mathcal{NB}$. The reason is that $P_i$ executes a step in which it increments every order variable ${order}_{ij}$ such that $P_j$ is not behind $P_i$.

Since $\mathcal{NB}$ is not empty following the first step of $P_i$, and since all processors that execute a step see the same set $\mathcal{NB}$, all the processors that execute a step following the first step of $P_i$ choose the same clock value. Thus, following the second step of $P_i$ and while $P_i$ does not stop to execute steps, the clock values of the processors that belong to $\mathcal{NB}$  are the same.

Every processor that executes a single step belongs to $\mathcal{NB}$, and the value of the clocks of all processors in $\mathcal{NB}$ is the same; thus the agreement requirement holds. Every processor chooses the maximal clock value $M$ of a processor in $\mathcal{NB}$ and increments $M$ by 1 modulo $M$; thus, the adjustment requirement holds as well.

The proof of the theorem assumes an arbitrary starting configuration for the execution with any combination of $order$ and $clock$ values. Thus our algorithm is both wait-free and self-stabilizing. (End)
