# 4.4 Update: Converting a Special Processor to an Id-based Dynamic System

In this section we present a self-stabilizing algorithm for the update task. The task of the most basic form of the update algorithm is to inform each processor of the identifiers of the processors that are in its *connected component*. Processors that can communicate directly or indirectly are in the same connected component. Therefore, every processor knows the maximal identifier in the system and a single leader is in fact elected. The self-stabilizing algorithm for the leader-election task given in section 2.9 can be used to convert self-stabilizing algorithms that are designed for systems with a unique leader to id-based systems.

The leader-election algorithm presented in section 2.9 stabilizes in $O(N)$ cycles, where $N$ is an upper bound on the number of processors in the entire system. This is a drawback in dynamic systems where link and processor failures can partition the system into several connected components, each with a small number of processors. A question to ask at this stage is whether there is a leader-election algorithm that stabilizes within $O(d)$ cycles, where $d$ is the actual diameter of the connected component. The self-stabilizing update algorithm that is presented here gives a positive answer to the above question.

Before describing the update algorithm, let us gain some intuition about the importance of a special processor in many self-stabilizing systems. Dijkstra presented the first self-stabilizing mutual-exclusion algorithm for a ring of processors in which all processors are identical except a single special processor. Dijkstra proved that, without this special processor, it is impossible to achieve mutual exclusion in a self-stabilizing manner. In distributed systems, it is difficult to guarantee that a special processor will always exist. Moreover, the assumption that such a special processor exists contradicts the distributive nature of the system. Active paths of research were triggered by this impossibility result. Self-stabilizing randomized leader election algorithms can be used to ensure that a single special processor, namely the elected leader, acts as the special processor. Other self-stabilizing distributed algorithms, such as the update algorithm presented in this section, assume that each processor has a unique identifier. Unique identifiers exist in several distributed systems, for instance in the widespread Ethernet local communication network.

Dijkstra's proof is for a composite (non-prime) number of processors connected in a ring and activated by the central daemon. Recall that the central daemon activates a single processor at a time and that the activated processor then reads the state of its neighbors and changes its state accordingly. A processor $P_i$ can execute the critical section in a configuration $c$ if, and only if, $P_i$ changes its state when it is the first processor to be activated by the central daemon following $c$. Therefore, every fair execution of a self-stabilizing mutual-exclusion algorithm must have a suffix in which exactly one processor may change a state in every configuration.

To demonstrate the technique for proving the impossibility result, we consider a special case in which the system is an oriented ring of four processors. Let $P_1$, $P_2$, $P_3$, $P_4$ be the processors in the ring, where $P_i$ is connected to $P_{i+1}$ for every $1 \leq i \leq 3$ and $P_4$ is connected to $P_1$. The ring is oriented so that $P_1$ is the left neighbor of $P_2$, $P_2$ is the left neighbor of $P_3$, $P_3$ is the left neighbor of $P_4$, and $P_4$ is the left neighbor of $P_1$. A processor $P_j$ is the right neighbor of a processor $P_i$ if $P_j$ is connected to $P_i$ and $P_j$ is not the left neighbor of $P_i$.

The proof considers executions that start in a symmetric configuration $c$ in which $P_1$ and $P_3$ are in the same state $s_0$ and $P_2$ and $P_4$ are in the same state $s'_0$. We show that there is an execution in which symmetry (between $P_1$ and $P_3$, and between $P_2$ and $P_4$) is never broken. The specific execution in which symmetry is preserved is the execution in which the processors repeatedly execute steps in the following order: $P_1$, $P_3$, $P_2$, $P_4$. Since there is no special processor, all the processors have the same transition function, denoted $f(s_l, s, s_r)$. The transition function $f(s_l, s, s_r)$ of a processor $P_i$ is a function of $s_l$, the state of the processor to the left of $P_i$, $s$, the state of $P_i$, and $s_r$, the state of the processor to the right of $P_i$.

In our execution, $P_1$ is the first activated processor following $c$, changing state according to $f$ and the state $s_l = s'_0$ of $P_4$, state $s = s_0$ of $P_1$, and state $s_r = s'_0$ of $P_2$. Denote the new state of $P_1$ by $s_1 = f(s'_0, s_0, s'_0)$. The second activated processor is $P_3$, which changes state according to the states $s_l = s'_0$ of $P_2$, $s = s_0$ of $P_3$ and $s_r = s'_0$ of $P_4$; therefore, $P_3$ changes its state to $s_1 = f(s'_0, s_0, s'_0)$. Similarly, $P_2$ is activated next and changes state to $s'_1 = f(s_1, s'_0, s_1)$, and so does $P_4$. Thus, in the configuration $c_4$ that follows the first activation of all the processors, $P_1$ and $P_3$ are in state $s_1$, while $P_2$ and $P_4$ are in state $s'_1$.

The proof is almost completed since, in the above execution, whenever $P_1$ may change a state from $s_m$ to $s_{m+1}$, $P_3$ may also change its state from $s_m$ to $s_{m+1}$. Analogously, the above holds for $P_2$ and $P_4$. Thus, in every configuration in which $P_1$ has permission to execute the critical section, $P_3$ has permission as well; and in every configuration in which $P_2$ may execute the critical section, $P_4$ may execute the critical section as well.

Note that when no central daemon exists, the impossibility result for the existence of a self-stabilizing mutual exclusion algorithm in a system of identical processors holds also for a ring of a prime number of processors. When we start in a configuration $c_0$ in which all the processors are in the same state — say $s_0$ — and the contents of the communication registers are identical, symmetry can be preserved forever. An execution that preserves symmetry is one in which every processor reads the communication registers of its neighbors before any processor writes a new value to its communication registers. Such a schedule guarantees that configurations in which all the states of the processors and the contents of registers are identical are reached infinitely often. Moreover, in the particular execution that we chose, each processor reads and writes the same values from/to the communication registers. Therefore, if a processor $P_i$ enters the critical section following a particular read or write operation, so does every processor that executes the same operation just before or just after $P_i$. Thus, there is no suffix of the execution in which, in every configuration, only one processor is in the critical section.

It is good practice to design an algorithm that is as general as possible; such an algorithm will fit a large class of systems. The impossibility results above motivate the restriction we make when designing the update algorithm: that each processor has a unique identifier.

In a distributed system, processors and communication links can fail and recover repeatedly. At any given time, a processor is connected directly or indirectly to a set of processors. Processors that can communicate directly or indirectly are in the same *connected component*. The update algorithm gathers information in a processor $P_i$ from all the processors in $P_i$'s connected component.

We define the task $UP$ to be the set of executions in which every processor has a set of identifiers consisting of an identifier for every processor in its connected component. Our update algorithm constructs n directed *BFS* trees. For every processor $P_i$, the algorithm constructs a directed *BFS* tree rooted at $P_i$. Roughly speaking, each *BFS* tree is a copy of the algorithm in section 2.5.

The update algorithm constructs the *first BFS tree* that is rooted at each processor: a graph may have more than a single *BFS* tree rooted at the same node. We define the *first BFS tree* of $G$ relative to $P_i$ to be a *BFS* tree rooted at $P_i$. When a node $P_j$ of distance $k+1$ from $P_i$ has more than one neighbor at distance $k$ from $P_i$, $P_j$ is connected to the neighbor with the maximal identifier among all its neighbors whose distance from $P_i$ is $k$ (other conventions such as the *last BFS tree* could be used as well, as long as the *BFS* tree is fixed).

Denote the upper bound on the number of processors in the system by $N$. During the execution of the algorithm, each processor $P_i$ maintains a set ${Processors}_i$ of no more than $N$ tuples $<id, dis>$. Each tuple in ${Processors}_i$ represents a processor in the system, where $id$ is the identifier of the processor and $dis$ is the distance (i.e., the number of edges) from $P_i$ to the processor with the identifier $id$. The value of the $id$ and the $dis$ fields is in the range 0 to $N$. $P_i$ communicates with any of its neighbors $P_j \in N(i)$ by writing (or sending) the value of ${Processors}_i$ and reading (or receiving, if message passing is used for communication) the value of ${Processors}_j$. In what follows, we describe the case when the processors communicate using shared memory. However, the same algorithm can be applied to message passing when each message arriving from $P_i$ to $P_j$ is written in an input buffer $r_{ij}$ of $P_j$.

Let $n \leq N$ be the number of the processors in the system. After the system stabilizes, it holds that ${Processors}_i$ contains $n$ tuples — a tuple $<j, x>$ for every processor $P_j$ in the system — such that $P_j$ is at distance $x$ from $P_i$.

A processor $P_i$ repeatedly reads the set ${Processors}_j$ of every neighbor $P_j$ and assigns ${Processors}_i$ by a set of tuples according to the sets $P_i$ reads. The tuples $P_i$ reads from its $\delta$ neighbors are stored in an internal set, ${ReadSet}_i$, of no more than $\delta N <id, dis>$ tuples.

The code of the algorithm appears in figure 4.6. $P_i$ repeatedly executes lines 2 through 10 of the code. $P_i$ initializes ${ReadSet}_i$ in line 2, and then $P_i$ accumulates the tuples in the sets of its neighbors into ${ReadSet}_i$ (lines 3 and 4). Note that the current contents of ${Processors}_i$ are not used for computing the new contents of ${Processors}_i$. The distance of $P_i$ to a processor $P_j \neq P_i$ is the distance of a processor $P_k$ to $P_j$, where $P_k$ is next to $P_i$, plus one; obviously, the distance of $P_i$ from itself is 0. In line 5, $P_i$ removes every tuple with an identifier field that is equal to the identifier of $P_i$, and then $P_i$ increments by 1 the distance of every remaining tuple (line 6). The semantics of the operators $\backslash \backslash$ and $++$ are implied in a straightforward manner by the above description. Next, $P_i$ adds the tuple $<i, 0>$ to ${ReadSet}_i$. In computing the distance from $P_i$ to $P_j$, $P_i$ believes a neighbor $P_k$ that is closest to $P_j$. In lines 8 and 9, for every processor $P_j$, $P_i$ removes each tuple with the identifier of $P_j$ except for the tuple with the smallest distance among these tuples.

![figure_4.6](images/figure_4.6.png)

An important issue in the design of the self-stabilizing update algorithm is how the algorithm rapidly eliminates floating tuples; a *floating tuple* is a tuple with an identifier of a processor that does not exist in the system. The operation $ConPrefix({ReadSet}_i)$ is the core technique used to eliminate floating tuples. Let $y$ be the minimal missing distance value in ${ReadSet}_i$. $ConPrefix({ReadSet}_i$) eliminates every tuple with a distance greater than $y$ in ${ReadSet}_i$. Intuitively, $ConPrefix$ eliminates tuples of processors that cannot be reached via a closer processor. Note that, in line 10, $P_i$ assigns to ${Processors}_i$, which may contain no more than $N$ tuples, a subset of ${ReadSet}_i$, which may contain $\delta N$ tuples. When the number of tuples assigned to ${Processors}_i$ in line 10 is greater than $N$, $P_i$ chooses $N$ tuples with the smallest distances among the tuples in ${ReadSet}_i$.

To prove the correctness of the algorithm, we define a *safe configuration*. A configuration is *safe* if, for every processor $P_i$, it holds that:

- ${Processors}_i$ includes n tuples, a tuple $<j, y>$ for every processor $P_j$ in the system, where $y$ is the distance of $P_j$ from $P_i$, and
- the tuples that are included in ${ReadSet}_i$ will cause $P_i$ to rewrite exactly the same contents to ${Processors}_i$.

According to our definition, in a safe configuration, every processor $P_i$ knows the set of processors in the system and the distance from each such processor. In addition, each tuple read by $P_i$ does not conflict with the knowledge of $P_i$. Therefore, in an execution that starts with a safe configuration, the value of ${Processors}_i$ is the same in every configuration.

Recall that, in every asynchronous cycle of an execution $E$, each processor executes at least one complete iteration of its do forever loop.

---

> LEMMA 4.6: In every arbitrary execution following the $k$ th cycle, it holds for all processors $P_i$ and $P_j$ that are at distance $l < min(k, d+1)$ that (1) a tuple $<j,l>$ appears in ${Processors}_i$, and (2) if a tuple $<x, y>$, such that $y \leq l$ appears in ${Processors}_i$, then there exists a processor $P_x$ at distance y from $P_i$.

*Proof:*

The proof is by induction on $k$, the number of cycles in the execution.

*Base case:*

(Proof for $k = 1$) During the first cycle, each processor executes line 7 of the code, adding the tuple $<i, 0>$ to ${ReadSet}_i$. By the increment operation of line 6 of the code, the distance of every tuple that is different from $<i, 0>$ in ${ReadSet}_i$ is greater than 0; therefore the tuple $<i, 0>$ is not eliminated in lines 8, 9 and 10. Thus, in the last write of $P_i$ to ${Processors}_i$ during the first cycle, the tuple $<i, 0>$ is written in ${Processors}_i$. Moreover, each further write operation follows the execution of lines 5 through 9 and therefore results in writing the tuple $<i, 0>$ to ${Processors}_i$. The above completes the proof of assertion (1) of the lemma. To prove assertion (2), we note that the distance field of every tuple is positive. Therefore, once the increment of line 6 is executed, no tuple of distance 0, except for $<i, 0>$, is written in ${Processors}_i$.

*Induction Step:*

We assume that, following the first $k$ cycles of the execution, assertions (1) and (2) hold for $l < min(k, d+1)$. We prove that, after one additional cycle, assertions (1) and (2) hold for $l < min(k+1, d+1)$.

By the induction assumption, following the first $k$ cycles of the execution, each tuple of distance $l < min(k, d+1)$ that should appear in ${Processors}_i$ does indeed appear there. Moreover, no tuple with distance $l < min(k, d + 1)$ that should not appear in ${Processors}_i$ appears in ${Processors}_i$. In other words, the Processors variables are correct up to distance $l − 1$.

In the $k+1$ cycle, every processor reads the tuples of its neighbors — in particular, it reads all the correct tuples with distances up to $l−1$. Therefore, every tuple of distance $l$ that is computed during the $k+1$ cycle is a correct tuple — indeed, a processor at distance $l$ with the identifier of the tuple exists. Moreover, since every tuple of distance $l−1$ appears following $k$ cycles, no tuple of distance $l$ is missing following the $k+1$ cycle. (End)

---

> LEMMA 4.7: In every arbitrary execution following $d+2$ cycles, it holds for every tuple $<x, y>$ in every ${Processors}_i$ variable that a processor $x$ exists in the system.

*Proof:*

In accordance with lemma 4.6, it holds that, following $d+1$ cycles, every tuple with distance $d$ or less is not a floating tuple; therefore, if a floating tuple $<x, y>$ (with a nonexistent identifier $x$) appears in a Processors variable, then $y > d$. During the cycle that follows the first $d+1$ cycle, every processor that reads the tuple $<x, y>$ increments $y$ (line 6 of the code) to be greater than $d+1$. The proof is complete, since no tuple of distance $d+1$ exists, and therefore the operation in line 10 of the code removes every floating tuple. (End)

---

> COROLLARY 4.1: In any execution, any configuration that follows the first $d+3$ cycles is a safe configuration.

*Proof:*

In accordance with lemma 4.6, it holds that, in every configuration that follows the first $d+1$ cycles, every tuple with distance $d$ or less is not a floating tuple; and for every two processors $P_i$ and $P_j$ at distance $l \leq d$, a tuple $<j,l>$ appears in ${Processors}_i$. In accordance with lemma 4.7, in every configuration that follows the first $d+2$ cycles, no tuple of greater distance exists in the Processors variables. Therefore, during the $d+3$ cycle, a safe configuration is reached immediately after every processor reads the tuples of its neighbors. (End)

## Self-Stabilizing Convergecast for Topology Update

We now show how the convergecast technique is composed with our update algorithm to yield a topology-update algorithm that stabilizes within $O(d)$ cycles. In the topology-update algorithm, the information that is convergecast is the local topology, i.e., the identity of the neighbors of each descendant. The local topology information may be collected through every tree or only through the tree of the processor with the maximal identifier, which we call the leader.

The convergecast mechanism assumes that every processor knows its parent and children in $T$, the *BFS* tree of the leader. Note that this assumption is valid after $O(d)$ cycles. The convergecast uses for every processor $P_i$ a variable ${up}_i$ in which $P_i$ writes to its parent in $T$. When $P_i$ is a leaf in $T$, $P_i$ writes its own local topology in ${up}_i$. Otherwise $P_i$ concatenates the values of the ${up}_i$ variables of all its children in $T$ and its own local topology, and writes the result in ${up}_i$. The stabilization of the convergecast mechanism is based on the correct information in the leaves and the direction in which information is collected, namely from the leaves toward the root of the tree.

Let the *height* of a processor $P_i$ in a rooted tree be the length of the longest path from $P_i$ to a leaf such that the path does not include the root. Obviously, following one cycle, the value of ${up}_i$ of every leaf processor is fixed and consists of its local topology. Therefore, following the second cycle of the execution, every processor whose children are leaves has fixed and correct topology information about its subtree. Similarly, following $h$ cycles, every processor of height $h−1$ or less has the correct topology on its subtree.

## Self-Stabilizing Broadcast for Topology Update

When the information concerning the topology is collected only on the tree of the leader, the convergecast results in correct data for the single leader. But, the rest of the processors do not know the topology. In order to inform every processor of the collected topology, we use a self-stabilizing broadcast mechanism. The broadcast uses for every processor $P_i$ a variable ${down}_i$ in which $P_i$ writes to its children. If $P_i$ is the leader, then $P_i$ repeatedly writes the value of ${up}_i$ in ${down}_i$. Otherwise, $P_i$ assigns the value of the $down$ variable of its parent to ${down}_i$. The stabilization of the broadcast mechanism is based on the fact that the (fixed) information that is broadcast is copied in a fixed direction from a parent to its children. Therefore, following the first cycle, after the root has the right information to broadcast, all of the root children have the correct information. Similarly, following the $i$ th cycle, every processor within distance $i$ from the root has the right information.

## Adaptive Self-Stabilizing Algorithms

The update algorithm is one of the best examples of a *memory-adaptive*, *time-adaptive*, and *communication-adaptive* self-stabilizing algorithm. In dynamic systems, the parameters of the system, such as the diameter, the number of processors, and the number of bits required to store the largest identifier in the system, are not fixed. For example, a link failure can partition the system into two independent connected components. Each connected component should stabilize independently and achieve its task. A self-stabilizing algorithm is *time-adaptive* if the number of cycles necessary to converge to a safe configuration is proportional to the actual parameters of the system, such as the *actual* diameter or *actual* number of processors in the system. Hence, the self-stabilizing leader election algorithm of section 2.9, which stabilizes in $O(N)$ cycles, is not time-adaptive, while the self-stabilizing update algorithm is time-adaptive.

A self-stabilizing algorithm is *memory-adaptive* if the amount of memory used in the system after a safe configuration is reached is proportional to the *actual* parameters of the system, such as the *actual* diameter or the *actual* number of processors. In contrast, a self-stabilizing algorithm is not memory-adaptive if the amount of memory used by the algorithm is proportional to an upper bound on the parameters of the system.

Recall that a silent self-stabilizing algorithm (see section 2.9) is one for which the communication between processors is fixed. A silent self-stabilizing algorithm designed for shared memory systems guarantees that, once a safe configuration is reached, the contents of the registers are fixed. A silent self-stabilizing algorithm designed for message passing systems guarantees that the only communication between any two neighboring processors that communicate between themselves is a single message that is sent repeatedly. A silent self-stabilizing algorithm is *communication-adaptive* if the number of bits that are (repeatedly) communicated between neighbors is proportional to the actual parameters of the system.

The self-stabilizing update algorithm stabilizes within $O(d)$ cycles and therefore is time-adaptive. The update algorithm repeatedly writes and reads $O(n)$ tuples in the communication registers, and is therefore memory-adaptive and communication-adaptive.
