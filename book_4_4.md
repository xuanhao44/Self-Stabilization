# 4.4 Update: Converting a Special Processor to an Id-based Dynamic System

In this section we present a self-stabilizing algorithm for the update task. The task of the most basic form of the update algorithm is to inform each processor of the identifiers of the processors that are in its *connected component*. Processors that can communicate directly or indirectly are in the same connected component. Therefore, every processor knows the maximal identifier in the system and a single leader is in fact elected. The self-stabilizing algorithm for the leader-election task given in section 2.9 can be used to convert self-stabilizing algorithms that are designed for systems with a unique leader to id-based systems.

The leader-election algorithm presented in section 2.9 stabilizes in O(N) cycles, where N is an upper bound on the number of processors in the entire system. This is a drawback in dynamic systems where link and processor failures can partition the system into several connected components, each with a small number of processors. A question to ask at this stage is whether there is a leader-election algorithm that stabilizes within O(d) cycles, where d is the actual diameter of the connected component. The self-stabilizing update algorithm that is presented here gives a positive answer to the above question.

Before describing the update algorithm, let us gain some intuition about the importance of a special processor in many self-stabilizing systems. Dijkstra presented the first self-stabilizing mutual-exclusion algorithm for a ring of processors in which all processors are identical except a single special processor. Dijkstra proved that, without this special processor, it is impossible to achieve mutual exclusion in a self-stabilizing manner. In distributed systems, it is difficult to guarantee that a special processor will always exist. Moreover, the assumption that such a special processor exists contradicts the distributive nature of the system. Active paths of research were triggered by this impossibility result. Self-stabilizing randomized leader election algorithms can be used to ensure that a single special processor, namely the elected leader, acts as the special processor. Other self-stabilizing distributed algorithms, such as the update algorithm presented in this section, assume that each processor has a unique identifier. Unique identifiers exist in several distributed systems, for instance in the widespread Ethernet local communication network.

Dijkstra's proof is for a composite (non-prime) number of processors connected in a ring and activated by the central daemon. Recall that the central daemon activates a single processor at a time and that the activated processor then reads the state of its neighbors and changes its state accordingly. A processor Pi can execute the critical section in a configuration c if, and only if, Pi changes its state when it is the first processor to be activated by the central daemon following c. Therefore, every fair execution of a self-stabilizing mutual-exclusion algorithm must have a suffix in which exactly one processor may change a state in every configuration.

To demonstrate the technique for proving the impossibility result, we consider a special case in which the system is an oriented ring of four processors. Let P1, P2, P3, P4 be the processors in the ring, where Pi is connected to Pi+1 for every 1 ≤ i ≤ 3 and P4 is connected to P1. The ring is oriented so that P1 is the left neighbor of P2, P2 is the left neighbor of P3, P3 is the left neighbor of P4, and P4 is the left neighbor of P1. A processor Pj is the right neighbor of a processor Pi if Pj is connected to Pi and Pj is not the left neighbor of Pi.

The proof considers executions that start in a symmetric configuration c in which P1 and P3 are in the same state s0 and P2 and P4 are in the same state s 0. We show that there is an execution in which symmetry (between P1 and P3, and between P2 and P4) is never broken. The specific execution in which symmetry is preserved is the execution in which the processors repeatedly execute steps in the following order: P1, P3, P2, P4. Since there is no special processor, all the processors have the same transition function, denoted f (sl,s,sr). The transition function f (sl,s,sr ) of a processor Pi is a function of sl, the state of the processor to the left of Pi , s, the state of Pi , and sr, the state of the processor to the right of Pi.

In our execution, P1 is the first activated processor following c, changing state according to f and the state sl = s 0 of P4, state s = s0 of P1, and state sr = s 0 of P2. Denote the new state of P1 by s1 = f (s 0,s0,s 0). The second activated processor is P3, which changes state according to the states sl = s 0 of P2, s = s0 of P3 and sr = s 0 of P4; therefore, P3 changes its state to s1 = f (s 0,s0,s 0). Similarly, P2 is activated next and changes state to s 1 = f (s1,s 0,s1), and so does P4. Thus, in the configuration c4 that follows the first activation of all the processors, P1 and P3 are in state s1, while P2 and P4 are in state s'1.

The proof is almost completed since, in the above execution, whenever P1 may change a state from sm to sm+1, P3 may also change its state from sm to sm+1. Analogously, the above holds for P2 and P4. Thus, in every configuration in which P1 has permission to execute the critical section, P3 has permission as well; and in every configuration in which P2 may execute the critical section, P4 may execute the critical section as well.

Note that when no central daemon exists, the impossibility result for the existence of a self-stabilizing mutual exclusion algorithm in a system of identical processors holds also for a ring of a prime number of processors. When we start in a configuration c0 in which all the processors are in the same state — say s0 — and the contents of the communication registers are identical, symmetry can be preserved forever. An execution that preserves symmetry is one in which every processor reads the communication registers of its neighbors before any processor writes a new value to its communication registers. Such a schedule guarantees that configurations in which all the states of the processors and the contents of registers are identical are reached infinitely often. Moreover, in the particular execution that we chose, each processor reads and writes the same values from/to the communication registers. Therefore, if a processor Pi enters the critical section following a particular read or write operation, so does every processor that executes the same operation just before or just after Pi . Thus, there is no suffix of the execution in which, in every configuration, only one processor is in the critical section.

It is good practice to design an algorithm that is as general as possible; such an algorithm will fit a large class of systems. The impossibility results above motivate the restriction we make when designing the update algorithm: that each processor has a unique identifier.

In a distributed system, processors and communication links can fail and recover repeatedly. At any given time, a processor is connected directly or indirectly to a set of processors. Processors that can communicate directly or indirectly are in the same connected component. The update algorithm gathers information in a processor Pi from all the processors in Pi’s connected component.

We define the task UP to be the set of executions in which every processor has a set of identifiers consisting of an identifier for every processor in its connected component. Our update algorithm constructs n directed BFS trees. For every processor Pi , the algorithm constructs a directed BFS tree rooted at Pi . Roughly speaking, each BFS tree is a copy of the algorithm in section 2.5.

The update algorithm constructs the first BFS tree that is rooted at each processor: a graph may have more than a single BFS tree rooted at the same node. We define the first BFS tree of G relative to Pi to be a BFS tree rooted at Pi . When a node Pj of distance k + 1 from Pi has more than one neighbor at distance k from Pi , Pj is connected to the neighbor with the maximal identifier among all its neighbors whose distance from Pi is k (other conventions such as the last BFS tree could be used as well, as long as the BFS tree is fixed).

Denote the upper bound on the number of processors in the system by N. During the execution of the algorithm, each processor Pi maintains a set Processorsi of no more than N tuples i d, dis	. Each tuple in Processorsi represents a processor in the system, where i d is the identifier of the processor and dis is the distance (i.e., the number of edges) from Pi to the processor with the identifier i d. The value of the i d and the dis fields is in the range 0 to N. Pi communicates with any of its neighbors Pj ∈ N(i) by writing (or sending) the value of Processorsi and reading (or receiving, if message passing is used for communication) the value of Processorsj . In what follows, we describe the case when the processors communicate using shared memory. However, the same algorithm can be applied to message passing when each message arriving from Pi to Pj is written in an input buffer rij of Pj.

Let n ≤ N be the number of the processors in the system. After the system stabilizes, it holds that Processorsi contains n tuples — a tuple j, x	 for every processor Pj in the system — such that Pj is at distance x from Pi . A processor Pi repeatedly reads the set Processorj of every neighbor Pj and assigns Processorsi by a set of tuples according to the sets Pi reads. The tuples Pi reads from its δ neighbors are stored in an internal set, ReadSeti, of no more than δN i d, dis	 tuples.

The code of the algorithm appears in figure 4.6. Pi repeatedly executes lines 2 through 10 of the code. Pi initializes ReadSeti in line 2, and then Pi accumulates the tuples in the sets of its neighbors into ReadSeti (lines 3 and 4). Note that the current contents of Processorsi are not used for computing the new contents of Processorsi . The distance of Pi to a processor Pj = Pi is the distance of a processor Pk to Pj , where Pk is next to Pi , plus one; obviously, the distance of Pi from itself is 0. In line 5, Pi removes every tuple with an identifier field that is equal to the identifier of Pi , and then Pi increments by 1 the distance of every remaining tuple (line 6). The semantics of the operators \\ and ++ are implied in a straightforward manner by the above description. Next, Pi adds the tuple i, 0	 to ReadSeti. In computing the distance from Pi to Pj , Pi believes a neighbor Pk that is closest to Pj . In lines 8 and 9, for every processor Pj , Pi removes each tuple with the identifier of Pj except for the tuple with the smallest distance among these tuples.

![figure_4.6](images/figure_4.6.png)

An important issue in the design of the self-stabilizing update algorithm is how the algorithm rapidly eliminates floating tuples; a floating tuple is a tuple with an identifier of a processor that does not exist in the system. The operation ConPrefix(ReadSeti) is the core technique used to eliminate floating tuples. Let y be the minimal missing distance value in ReadSeti. ConPrefix(ReadSeti) eliminates every tuple with a distance greater than y in ReadSeti. Intuitively, ConPrefix eliminates tuples of processors that cannot be reached via a closer processor. Note that, in line 10, Pi assigns to Processorsi , which may contain no more than N tuples, a subset of ReadSeti, which may contain δN tuples. When the number of tuples assigned to Processorsi in line 10 is greater than N, Pi chooses N tuples with the smallest distances among the tuples in ReadSeti.

To prove the correctness of the algorithm, we define a safe configuration. A configuration is safe if, for every processor Pi , it holds that:

- Processorsi includes n tuples, a tuple j, y	 for every processor Pj in the system, where y is the distance of Pj from Pi , and
- the tuples that are included in ReadSeti will cause Pi to rewrite exactly the same contents to Processorsi.

According to our definition, in a safe configuration, every processor Pi knows the set of processors in the system and the distance from each such processor. In addition, each tuple read by Pi does not conflict with the knowledge of Pi . Therefore, in an execution that starts with a safe configuration, the value of Processorsi is the same in every configuration.

Recall that, in every asynchronous cycle of an execution E, each processor executes at least one complete iteration of its do forever loop.

---

> LEMMA 4.6: In every arbitrary execution following the kth cycle, it holds for all processors Pi and Pj that are at distance l < min(k, d + 1) that (1) a tuple j,l	 appears in Processorsi , and (2) if a tuple x, y	, such that y ≤ l appears in Processorsi , then there exists a processor Px at distance y from Pi .

*Proof:*

The proof is by induction on k, the number of cycles in the execution.

*Base case:*

(Proof for k = 1) During the first cycle, each processor executes line 7 of the code, adding the tuple i, 0	 to ReadSeti. By the increment operation of line 6 of the code, the distance of every tuple that is different from i, 0	 in ReadSeti is greater than 0; therefore the tuple i, 0	 is not eliminated in lines 8, 9 and 10. Thus, in the last write of Pi to Processorsi during the first cycle, the tuple i, 0	 is written in Processorsi . Moreover, each further write operation follows the execution of lines 5 through 9 and therefore results in writing the tuple i, 0	 to Processorsi . The above completes the proof of assertion (1) of the lemma. To prove assertion (2), we note that the distance field of every tuple is positive. Therefore, once the increment of line 6 is executed, no tuple of distance 0, except for i, 0	, is written in Processorsi .

*Induction Step:*

We assume that, following the first k cycles of the execution, assertions (1) and (2) hold for l < min(k, d + 1). We prove that, after one additional cycle, assertions (1) and (2) hold for l < min(k + 1, d + 1).

By the induction assumption, following the first k cycles of the execution, each tuple of distance l < min(k, d + 1) that should appear in Processorsi does indeed appear there. Moreover, no tuple with distance l < min(k, d + 1) that should not appear in Processorsi appears in Processorsi . In other words, the Processors variables are correct up to distance l − 1.

In the k + 1 cycle, every processor reads the tuples of its neighbors — in particular, it reads all the correct tuples with distances up to l − 1. Therefore, every tuple of distance l that is computed during the k + 1 cycle is a correct tuple — indeed, a processor at distance l with the identifier of the tuple exists. Moreover, since every tuple of distance l − 1 appears following k cycles, no tuple of distance l is missing following the k + 1 cycle. (End)

---

> LEMMA 4.7: In every arbitrary execution following d + 2 cycles, it holds for every tuple x, y	 in every Processorsi variable that a processor x exists in the system.

*Proof:*

In accordance with lemma 4.6, it holds that, following d + 1 cycles, every tuple with distance d or less is not a floating tuple; therefore, if a floating tuple x, y	 (with a nonexistent identifier x) appears in a Processors variable, then y > d. During the cycle that follows the first d + 1 cycle, every processor that reads the tuple x, y	 increments y (line 6 of the code) to be greater than d + 1. The proof is complete, since no tuple of distance d + 1 exists, and therefore the operation in line 10 of the code removes every floating tuple. (End)

---

> COROLLARY 4.1: In any execution, any configuration that follows the first d + 3 cycles is a safe configuration.

*Proof:*

In accordance with lemma 4.6, it holds that, in every configuration that follows the first d+1 cycles, every tuple with distance d or less is not a floating tuple; and for every two processors Pi and Pj at distance l ≤ d, a tuple j,l	 appears in Processorsi . In accordance with lemma 4.7, in every configuration that follows the first d + 2 cycles, no tuple of greater distance exists in the Processors variables. Therefore, during the d + 3 cycle, a safe configuration is reached immediately after every processor reads the tuples of its neighbors. (End)

## Self-Stabilizing Convergecast for Topology Update

We now show how the convergecast technique is composed with our update algorithm to yield a topology-update algorithm that stabilizes within O(d) cycles. In the topology-update algorithm, the information that is convergecast is the local topology, i.e., the identity of the neighbors of each descendant. The local topology information may be collected through every tree or only through the tree of the processor with the maximal identifier, which we call the leader.

The convergecast mechanism assumes that every processor knows its parent and children in T , the BFS tree of the leader. Note that this assumption is valid after O(d) cycles. The convergecast uses for every processor Pi a variable upi in which Pi writes to its parent in T . When Pi is a leaf in T , Pi writes its own local topology in upi. Otherwise Pi concatenates the values of the upi variables of all its children in T and its own local topology, and writes the result in upi. The stabilization of the convergecast mechanism is based on the correct information in the leaves and the direction in which information is collected, namely from the leaves toward the root of the tree.

Let the height of a processor Pi in a rooted tree be the length of the longest path from Pi to a leaf such that the path does not include the root. Obviously, following one cycle, the value of upi of every leaf processor is fixed and consists of its local topology. Therefore, following the second cycle of the execution, every processor whose children are leaves has fixed and correct topology information about its subtree. Similarly, following h cycles, every processor of height h − 1 or less has the correct topology on its subtree.

## Self-Stabilizing Broadcast for Topology Update

When the information concerning the topology is collected only on the tree of the leader, the convergecast results in correct data for the single leader. But, the rest of the processors do not know the topology. In order to inform every processor of the collected topology, we use a self-stabilizing broadcast mechanism. The broadcast uses for every processor Pi a variable downi in which Pi writes to its children. If Pi is the leader, then Pi repeatedly writes the value of upi in downi . Otherwise, Pi assigns the value of the down variable of its parent to downi . The stabilization of the broadcast mechanism is based on the fact that the (fixed) information that is broadcast is copied in a fixed direction from a parent to its children. Therefore, following the first cycle, after the root has the right information to broadcast, all of the root children have the correct information. Similarly, following the ith cycle, every processor within distance i from the root has the right information.

## Adaptive Self-Stabilizing Algorithms

The update algorithm is one of the best examples of a memory-adaptive, time-adaptive, and communication-adaptive self-stabilizing algorithm. In dynamic systems, the parameters of the system, such as the diameter, the number of processors, and the number of bits required to store the largest identifier in the system, are not fixed. For example, a link failure can partition the system into two independent connected components. Each connected component should stabilize independently and achieve its task. A self-stabilizing algorithm is time-adaptive if the number of cycles necessary to converge to a safe configuration is proportional to the actual parameters of the system, such as the actual diameter or actual number of processors in the system. Hence, the self-stabilizing leader election algorithm of section 2.9, which stabilizes in O(N) cycles, is not time-adaptive, while the self-stabilizing update algorithm is time-adaptive.

A self-stabilizing algorithm is memory-adaptive if the amount of memory used in the system after a safe configuration is reached is proportional to the actual parameters of the system, such as the actual diameter or the actual number of processors. In contrast, a self-stabilizing algorithm is not memory-adaptive if the amount of memory used by the algorithm is proportional to an upper bound on the parameters of the system.

Recall that a silent self-stabilizing algorithm (see section 2.9) is one for which the communication between processors is fixed. A silent self-stabilizing algorithm designed for shared memory systems guarantees that, once a safe configuration is reached, the contents of the registers are fixed. A silent selfstabilizing algorithm designed for message passing systems guarantees that the only communication between any two neighboring processors that communicate between themselves is a single message that is sent repeatedly. A silent self-stabilizing algorithm is communication-adaptive if the number of bits that are (repeatedly) communicated between neighbors is proportional to the actual parameters of the system.

The self-stabilizing update algorithm stabilizes within O(d) cycles and therefore is time-adaptive. The update algorithm repeatedly writes and reads O(n) tuples in the communication registers, and is therefore memory-adaptive and communication-adaptive.